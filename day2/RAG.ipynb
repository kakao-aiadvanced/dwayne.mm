{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T07:26:23.201061Z",
     "start_time": "2025-03-25T07:26:16.862971Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "import getpass\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef87886a8b55cf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T07:26:27.097140Z",
     "start_time": "2025-03-25T07:26:25.190314Z"
    }
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(urls),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 6}\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925761cb9e1fda38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T07:26:38.657534Z",
     "start_time": "2025-03-25T07:26:30.367255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STEP1] relevance check result: {'relevance': 'yes'}\n",
      "\n",
      "\n",
      "[STEP2] 최종 답변:\n",
      "Agent memory consists of both short-term and long-term components, allowing the agent to retain and recall information over varying time frames:\n",
      "\n",
      "1. **Short-term Memory**: This involves in-context learning, where the agent utilizes the immediate information available within a specific interaction to make decisions. It allows the agent to react to situations based on recent inputs and context but does not retain this information for long periods.\n",
      "\n",
      "2. **Long-term Memory**: This module enables the agent to retain and recall an infinite amount of information over extended periods. It relies on external databases or vector stores to organize and retrieve memories efficiently. Long-term memory captures a comprehensive list of experiences and interactions, providing a foundation for the agent's behavior informed by past events.\n",
      "\n",
      "Together, these memory systems help agents synthesize past experiences into higher-level summaries, guiding their future actions and decision-making processes.\n",
      "\n",
      "\n",
      "[STEP3] 출처:\n",
      "- https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "- https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "- https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "- https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "- https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "- https://lilianweng.github.io/posts/2023-06-23-agent/\n"
     ]
    }
   ],
   "source": [
    "query = \"agent memory\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "context_text = format_docs(retrieved_docs)\n",
    "\n",
    "evaluation_template = (\n",
    "    \"retrieval 퀄리티를 LLM이 스스로 평가하도록 하세요. 문맥이 사용자 쿼리와 관련이 있으면 yes, 관련이 없으면 no로 출력합니다. relevance : yes, no 로 출력해주세요.\\n\"\n",
    "    \"{format_instructions}\\n\"\n",
    "    \"Context: {context}\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    ")\n",
    "evaluation_prompt = PromptTemplate(\n",
    "    template=evaluation_template,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": JsonOutputParser().get_format_instructions()},\n",
    ")\n",
    "\n",
    "evaluation_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | evaluation_prompt\n",
    "        | llm\n",
    "        | JsonOutputParser()\n",
    ")\n",
    "\n",
    "evaluation_result = evaluation_chain.invoke(query)\n",
    "print(\"[STEP1] relevance check result:\", evaluation_result)\n",
    "\n",
    "answer = \"\"\n",
    "regenerated = False\n",
    "if evaluation_result.get(\"relevance\") == \"yes\":\n",
    "    for _ in range(2):\n",
    "        answer_template = (\n",
    "            \"다음 문맥을 기반으로 사용자 질문에 답변해 주세요.\\n\"\n",
    "            \"Context: {context}\\n\"\n",
    "            \"Question: {question}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        answer_prompt = PromptTemplate(\n",
    "            template=answer_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "        rag_chain = (\n",
    "                {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                | answer_prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        for chunk in rag_chain.stream(query):\n",
    "            answer += chunk\n",
    "\n",
    "        hallucination_template = (\n",
    "            \"생성된 답안에 Hallucination(잘못된 정보나 근거 없는 내용)이 포함되어 있는지 평가하세요. 반드시 유효한 JSON 형식으로만 답변하세요. 만약 답안에 사실과 다르거나 허위 내용이 있다면 yes, 허위 내용이 없다면 no로 출력합니다. hallucination : yes, no 로 출력해주세요.\\n\"\n",
    "            \"{format_instructions}\\n\"\n",
    "            \"Context: {context}\\n\"\n",
    "            \"Answer: {answer}\\n\"\n",
    "        )\n",
    "        hallucination_prompt = PromptTemplate(\n",
    "            template=hallucination_template,\n",
    "            input_variables=[\"context\", \"answer\"],\n",
    "            partial_variables={\"format_instructions\": JsonOutputParser().get_format_instructions()},\n",
    "        )\n",
    "        hallucination_chain = hallucination_prompt | llm | JsonOutputParser()\n",
    "        hallucination_result = hallucination_chain.invoke({\"context\": context_text, \"answer\": answer})\n",
    "\n",
    "        #hallucination_result['hallucination'] = \"yes\" (할루시네이션 테스트)\n",
    "\n",
    "        if hallucination_result.get(\"hallucination\") == \"no\":\n",
    "            break\n",
    "        elif hallucination_result.get(\"hallucination\") == \"yes\":\n",
    "            if regenerated:\n",
    "                print(\"\\n최대 재생성 횟수(1회)를 초과\\n\")\n",
    "                answer = \"\"\n",
    "                break\n",
    "            else:\n",
    "                print(\"\\nhallucination 감지됨: 답변을 다시 생성합니다.\\n\")\n",
    "                regenerated = True\n",
    "else:\n",
    "    print(\"\\nRetrieved chunks are not relevant to the query.\")\n",
    "\n",
    "####### RESULT ######\n",
    "if answer != \"\":\n",
    "    print(\"\\n\\n[STEP2] 최종 답변:\")\n",
    "    print(answer)\n",
    "    print(\"\\n\\n[STEP3] 출처:\")\n",
    "    for doc in retrieved_docs:\n",
    "        source = doc.metadata.get(\"source\", \"출처 정보 없음\") if hasattr(doc, \"metadata\") else \"출처 정보 없음\"\n",
    "        print(\"-\", source)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
